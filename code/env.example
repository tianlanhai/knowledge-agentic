# ============================================================================
# 知识库智能体 - 环境变量配置示例
# ============================================================================
# 使用说明：
# 1. 复制此文件为 .env
# 2. 根据实际环境修改配置项
# 3. .env 文件不会被 git 追踪，可用于存储敏感信息
# ============================================================================

# ----------------------------------------------------------------------------
# 应用基础配置
# ----------------------------------------------------------------------------
# 应用名称（默认：Knowledge-Agentic）
# APP_NAME=Knowledge-Agentic

# 应用版本（默认：0.1.0）
# APP_VERSION=0.1.0

# API 版本前缀（默认：/api/v1）
# API_V1_STR=/api/v1

# 服务端口（默认：8010）
# PORT=8010

# 日志级别（默认：INFO，可选：DEBUG、INFO、WARNING、ERROR）
LOG_LEVEL=INFO

# ----------------------------------------------------------------------------
# CORS 配置
# ----------------------------------------------------------------------------

# 允许的跨域来源列表，用逗号分隔（示例）
# BACKEND_CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173

# ----------------------------------------------------------------------------
# LLM 模型提供商配置
# ----------------------------------------------------------------------------
# LLM 提供商选择（默认：ollama）
# 可选值：ollama（本地）、zhipuai（智谱AI）、minimax（MiniMax）、moonshot（月之暗面）
# 根据选择的不同提供商，需要配置对应的API密钥
LLM_PROVIDER=ollama

# ----------------------------------------------------------------------------
# Ollama 配置
# ----------------------------------------------------------------------------
# 允许的跨域来源列表，用逗号分隔（示例）
# BACKEND_CORS_ORIGINS=http://localhost:5173,http://127.0.0.1:5173

# ----------------------------------------------------------------------------
# Ollama 配置
# ----------------------------------------------------------------------------
# Ollama 服务地址（默认：http://localhost:11434）
# OLLAMA_BASE_URL=http://localhost:11434

# 嵌入模型名称（默认：mxbai-embed-large:latest）
EMBEDDING_MODEL=mxbai-embed-large:latest

# 对话模型名称（默认：deepseek-r1:8b）
CHAT_MODEL=deepseek-r1:8b

# 使用的 GPU 数量（默认：1，0 表示使用 CPU）
# Ollama 会优先使用 GPU，如果设为 0 则强制使用 CPU
OLLAMA_NUM_GPU=1

# GPU 内存利用率（默认：0.9，范围 0-1）
# 设置为 0.9 表示使用 90% 的 GPU 显存
# 如果遇到显存不足错误，可降低此值到 0.7 或 0.5
# OLLAMA_GPU_MEMORY_UTILIZATION=0.9

# ----------------------------------------------------------------------------
# 智谱AI 配置（支持LLM和Embedding分离配置）
# ----------------------------------------------------------------------------
# 智谱AI API 密钥（通用配置，作为默认值）
# 获取方式：访问 https://open.bigmodel.cn/usercenter/apikeys
# ZHIPUAI_API_KEY=your_zhipuai_api_key_here

# 智谱AI API基础地址（通用配置，可选，默认使用官方地址）
# 如需使用代理地址，可修改此配置，例如：http://your-proxy.com/api/paas/v4
# ZHIPUAI_BASE_URL=https://open.bigmodel.cn/api/paas/v4

# 智谱AI LLM 独立配置（优先于通用配置）
# ZHIPUAI_LLM_API_KEY=your_llm_api_key_here
# ZHIPUAI_LLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4

# 智谱AI Embedding 独立配置（优先于通用配置）
# ZHIPUAI_EMBEDDING_API_KEY=your_embedding_api_key_here
# ZHIPUAI_EMBEDDING_BASE_URL=https://open.bigmodel.cn/api/paas/v4

# 智谱AI 模型名称（默认：glm-4.5-air）
# 可选模型：
#   - glm-4.5-air: GLM-4.5-Air（推荐，速度快、成本低）
#   - glm-4: GLM-4（功能强大）
#   - glm-4-flash: GLM-4 Flash（快速响应）
#   - glm-3-turbo: GLM-3 Turbo（经济实惠）
# ZHIPUAI_MODEL=glm-4.5-air

# 智谱AI Embedding 模型名称（默认：embedding-3）
# ZHIPUAI_EMBEDDING_MODEL=embedding-3

# ----------------------------------------------------------------------------
# MiniMax 配置
# ----------------------------------------------------------------------------
# MiniMax API 密钥（必须）
# 获取方式：访问 https://api.minimax.chat/document/guides/chat 获取
# MINIMAX_API_KEY=your_minimax_api_key_here

# MiniMax Group ID（必须）
# 获取方式：同上，在创建应用后获得
# MINIMAX_GROUP_ID=your_minimax_group_id_here

# MiniMax 模型名称（默认：abab5.5-chat）
# 可选模型：abab5.5-chat、abab5.5s-chat
# MINIMAX_MODEL=abab5.5-chat

# ----------------------------------------------------------------------------
# 月之暗面（Moonshot）配置
# ----------------------------------------------------------------------------
# 月之暗面 API 密钥（必须）
# 获取方式：访问 https://platform.moonshot.cn/console/api-keys
# MOONSHOT_API_KEY=your_moonshot_api_key_here

# 月之暗面模型名称（默认：moonshot-v1-8k）
# 可选模型：moonshot-v1-8k、moonshot-v1-32k、moonshot-v1-128k
# MOONSHOT_MODEL=moonshot-v1-8k

# ----------------------------------------------------------------------------
# 本地向量化配置（可选）
# ----------------------------------------------------------------------------
# 是否使用本地嵌入模型（默认：False）
# True: 使用 HuggingFace 本地模型（BAAI/bge-large-zh-v1.5），需要安装 torch
# False: 使用 Ollama API（mxbai-embed-large:latest）
# USE_LOCAL_EMBEDDINGS=False

# 设备类型（默认：auto）
# auto: 自动检测（优先 CUDA，无 GPU 则使用 CPU）
# cuda: 强制使用 CUDA（需要 GPU 和 CUDA 工具包）
# cpu: 强制使用 CPU
DEVICE=auto

# 本地嵌入模型路径或 HuggingFace 标识（默认：BAAI/bge-large-zh-v1.5）
# 如果使用本地模型，需要先运行 download_embedding_model.py 下载
# LOCAL_EMBEDDING_MODEL_PATH=BAAI/bge-large-zh-v1.5

# 本地模型存储目录（默认：./models）
# 用于扫描本地可用的Embedding模型，每个模型存储在一个子目录中
# 例如：./models/bge-large-zh-v1.5/, ./models/bge-small-zh-v1.5/
# LOCAL_MODEL_DIR=./models

# ----------------------------------------------------------------------------
# 数据库配置（多供应商支持）
# ----------------------------------------------------------------------------
# 数据库提供商（默认：postgresql）
# 可选值：
#   - postgresql: PostgreSQL数据库（推荐生产环境）
#   - sqlite: SQLite文件数据库（开发环境，无需额外服务）
DATABASE_PROVIDER=postgresql

# PostgreSQL 配置（当 DATABASE_PROVIDER=postgresql 时使用）
# 本地开发环境
# DB_HOST=localhost
# DB_PORT=5432
# DB_NAME=knowledge_db
# DB_USER=postgres
# DB_PASSWORD=your_password_here

# Docker 部署环境（访问宿主机数据库）
# DB_HOST=host.docker.internal
# DB_PORT=15432
# DB_NAME=knowledge_db
# DB_USER=postgres
# DB_PASSWORD=your_password_here

# SQLite 配置（当 DATABASE_PROVIDER=sqlite 时使用）
# SQLITE_DB_PATH=./data/metadata.db

# ----------------------------------------------------------------------------
# 向量数据库配置
# ----------------------------------------------------------------------------
# ChromaDB 存储路径（默认：./data/chroma_db）
# 向量数据库存储目录（本地存储，不支持多供应商）
CHROMA_DB_PATH=./data/chroma_db

# ChromaDB 集合名称（默认：knowledge_base）
CHROMA_COLLECTION_NAME=knowledge_base

# ----------------------------------------------------------------------------
# 调试与 Mock 配置
# ----------------------------------------------------------------------------
# 是否使用 Mock 模式（默认：False）
# True: 不调用真实模型，返回模拟数据，用于快速测试
# False: 调用真实的 Ollama 模型
# USE_MOCK=False

# ============================================================================
# 开发环境推荐配置
# ============================================================================

# 快速开发模式（使用 Ollama API）
# LOG_LEVEL=DEBUG
# LLM_PROVIDER=ollama
# OLLAMA_NUM_GPU=1
# USE_MOCK=False

# Mock 测试模式（无需 Ollama）
# LOG_LEVEL=DEBUG
# LLM_PROVIDER=ollama
# USE_MOCK=True

# 完整 GPU 加速模式（本地嵌入 + Ollama GPU）
# LOG_LEVEL=DEBUG
# LLM_PROVIDER=ollama
# OLLAMA_NUM_GPU=1
# USE_LOCAL_EMBEDDINGS=True
# DEVICE=cuda

# 智谱AI 模式（使用智谱AI云端模型）
# LOG_LEVEL=DEBUG
# LLM_PROVIDER=zhipuai
# ZHIPUAI_API_KEY=your_zhipuai_api_key
# ZHIPUAI_MODEL=glm-4-air
# USE_LOCAL_EMBEDDINGS=True
# DEVICE=cpu
# LOCAL_EMBEDDING_MODEL_PATH=/app/models/bge-large-zh-v1.5

# Docker部署模式（智谱AI + 本地向量模型）
# LOG_LEVEL=INFO
# LLM_PROVIDER=zhipuai
# ZHIPUAI_API_KEY=your_zhipuai_api_key
# ZHIPUAI_MODEL=glm-4-air
# USE_LOCAL_EMBEDDINGS=True
# DEVICE=cpu
# LOCAL_EMBEDDING_MODEL_PATH=/app/models/bge-large-zh-v1.5

# MiniMax 模式（使用 MiniMax 云端模型）
# LOG_LEVEL=DEBUG
# LLM_PROVIDER=minimax
# MINIMAX_API_KEY=your_minimax_api_key
# MINIMAX_GROUP_ID=your_minimax_group_id
# MINIMAX_MODEL=abab5.5-chat

# 月之暗面模式（使用 Moonshot 云端模型）
# LOG_LEVEL=DEBUG
# LLM_PROVIDER=moonshot
# MOONSHOT_API_KEY=your_moonshot_api_key
# MOONSHOT_MODEL=moonshot-v1-8k

# ============================================================================
# 注意事项
# ============================================================================
# 1. 修改配置后需要重启服务才能生效
# 2. .env 文件中的注释行（以 # 开头）会被忽略
# 3. 空值配置项会使用代码中的默认值
# 4. GPU 配置需要预先安装 CUDA 工具包（建议 11.8 或 12.1）
# 5. 本地向量化模式需要安装 torch 和 sentence-transformers
# 6. 使用云端模型（智谱AI、MiniMax、月之暗面）需要配置对应的API密钥
# 7. API密钥获取：
#    - 智谱AI：https://open.bigmodel.cn/usercenter/apikeys
#    - MiniMax：https://api.minimax.chat/document/guides/chat
#    - 月之暗面：https://platform.moonshot.cn/console/api-keys
# 8. 切换模型提供商时，只需修改 LLM_PROVIDER 配置项并重启服务
# ============================================================================
