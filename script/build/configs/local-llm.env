# ============================================================================
# 本地LLM配置片段（Ollama）
# File    : configs/local-llm.env
# Purpose : 适用于v3、v4版本（本地LLM）
# ============================================================================

# ----------------------------------------------------------------------------
# LLM提供商配置
# ----------------------------------------------------------------------------
LLM_PROVIDER=ollama

# ----------------------------------------------------------------------------
# Ollama服务配置
# ----------------------------------------------------------------------------
OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
CHAT_MODEL=deepseek-r1:8b
OLLAMA_NUM_GPU=1
OLLAMA_GPU_MEMORY_UTILIZATION=0.9
