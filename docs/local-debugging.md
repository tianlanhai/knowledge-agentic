# 知识库智能体 - 本地调试文档

## 1. 调试环境准备

帅哥，为了在开发过程中快速排查问题，我们需要配置高效的本地调试环境。

### 1.1 环境变量配置

在项目根目录创建 `.env` 文件，开启调试模式：
- `DEBUG=True`
- `LOG_LEVEL=DEBUG`
- `OLLAMA_HOST=127.0.0.1:11434`

参考 `.env.example` 文件查看所有可配置项。

### 1.2 GPU 加速配置

若要利用 GPU 加速，请确保以下环境：

#### 1.2.1 Ollama GPU 配置

Ollama 支持自动检测并使用 GPU，也可通过环境变量进行精确配置。

**自动检测模式（推荐）：**
```bash
# Ollama 会自动使用可用的 GPU
# 无需额外配置
```

**手动配置模式：**
```bash
# 在 .env 文件中添加
OLLAMA_NUM_GPU=1              # 使用的 GPU 数量（0表示使用CPU）
OLLAMA_GPU_MEMORY_UTILIZATION=0.9  # GPU 内存利用率（0-1，默认0.9）
```

**验证 Ollama GPU 是否生效：**
```bash
# 1. 查看已安装的模型
ollama list

# 2. 运行测试模型并观察 GPU 使用情况
ollama run deepseek-r1:8b

# 3. 使用任务管理器或 nvidia-smi 查看 GPU 显存占用
nvidia-smi  # Linux/Windows WSL
```

**常见 GPU 问题排查：**

| 问题 | 可能原因 | 解决方案 |
|------|---------|---------|
| GPU 使用率 0% | CUDA 未安装 | 安装 CUDA Toolkit 11.8+ |
| 显存不足 | 模型太大 | 调整 `OLLAMA_GPU_MEMORY_UTILIZATION` 为 0.7 或更低 |
| 推理速度慢 | 实际使用 CPU | 设置 `OLLAMA_NUM_GPU=1` 并重启服务 |

#### 1.2.2 本地嵌入模型加速

- 设置 `USE_LOCAL_EMBEDDINGS=True`
- 设置 `DEVICE=cuda` (或 `auto` 自动检测)
- 首次运行会自动从 HuggingFace 下载模型至本地缓存

### 1.3 推荐 IDE 配置

- **断点调试**：在 `.vscode/launch.json` 中配置 FastAPI 调试任务，支持在 API 处理函数中设置断点。
- **代码检查**：启用 `flake8` 或 `black` 自动格式化，保持代码风格一致。

## 2. 启动命令

帅哥，配置好环境后，按照以下步骤启动服务：

### 2.1 启动 Ollama 服务

确保 Ollama 服务已启动并下载所需模型：
```bash
# 启动 Ollama 服务（Windows 下通常自动启动）
# 验证服务是否运行
ollama list

# 下载所需模型（首次运行）
ollama pull deepseek-r1:8b
ollama pull mxbai-embed-large:latest
```

### 2.2 启动后端服务

进入后端目录并启动 FastAPI 服务：
```bash
# 进入后端目录
cd code

# 方式一：使用 uvicorn 启动（推荐，支持热重载）
uvicorn app.main:app --host 0.0.0.0 --port 8010 --reload

# 方式二：直接运行主入口文件
python -m app.main
```

后端启动成功后：
- API 访问地址：`http://127.0.0.1:8010`
- Swagger 文档：`http://127.0.0.1:8010/docs`
- 健康检查：`http://127.0.0.1:8010/`

### 2.3 启动前端服务

进入前端目录并启动 Vite 开发服务器：
```bash
# 进入前端目录
cd frontend

# 安装依赖（首次运行）
pnpm install

# 启动开发服务器
pnpm dev
```

前端启动成功后：
- 前端访问地址：`http://localhost:5173`

### 2.4 完整启动流程

建议使用两个终端窗口分别启动后端和前端：

**终端 1 - 启动后端：**
```bash
cd code
uvicorn app.main:app --host 0.0.0.0 --port 8010 --reload
```

**终端 2 - 启动前端：**
```bash
cd frontend
pnpm dev
```

### 2.5 验证服务状态

启动完成后，按以下步骤验证：
1. 访问 `http://127.0.0.1:8010/` 查看后端健康状态
2. 访问 `http://127.0.0.1:8010/docs` 测试 API 接口
3. 访问 `http://localhost:5173` 查看前端界面
4. 检查终端日志，确认无错误信息

## 3. 文件存储配置

### 3.1 上传文件存储

- 文件上传后保存到 `code/data/files/` 目录
- 使用 UUID 生成唯一文件名，防止同名文件覆盖
- 文件元数据存储在 SQLite 数据库中

### 3.2 数据存储路径

- SQLite 数据库：`code/data/metadata.db`
- ChromaDB 向量库：`code/data/chroma_db`

## 4. 日志分析

调试过程中请重点关注以下日志：

### 4.1 API 请求日志

- 记录每个接口的入参、响应时间和状态码
- 查看请求是否成功发送
- 检查响应时间是否合理

### 4.2 检索增强生成检索日志

- 打印从向量库检索出的片段内容及评分
- 检查检索结果是否相关
- 查看检索片段数量

### 4.3 智能体思考过程

- 打印 LangGraph 的节点流转和中间思考结果
- 查看任务拆解是否合理
- 检查工具调用是否正确

## 5. 常用调试工具

### 5.1 交互式 Swagger

访问 `http://127.0.0.1:8010/docs` 直接进行接口联调。

**功能：**
- 查看所有 API 接口
- 在线测试接口
- 查看请求和响应示例
- 下载 OpenAPI 规范

### 5.2 Postman

使用 Postman 进行 API 测试。

**功能：**
- 保存请求集合
- 环境变量管理
- 自动化测试
- 文档生成

## 6. 常见调试场景与排查步骤

### 6.1 后端调试

#### 6.1.1 检索增强生成回答不准

**排查步骤：**
1. 检查日志中的检索片段是否包含正确信息
2. 检查切分逻辑是否导致上下文断裂
3. 检查嵌入模型是否匹配
4. 调整检索参数（top_k、相似度阈值）
5. 优化提示词模板

#### 6.1.2 智能体陷入死循环

**排查步骤：**
1. 查看 LangGraph 状态图日志
2. 检查停止条件是否正确触发
3. 增加最大迭代次数限制
4. 优化任务拆解逻辑

#### 6.1.3 内存占用过高

**排查步骤：**
1. 确认是否同时启动了多个模型
2. 检查向量数据库是否在频繁刷盘
3. 减少批处理大小
4. 启用向量库压缩

### 6.2 网络调试

#### 6.2.1 接口请求失败

**排查步骤：**
1. 使用 Postman 或 curl 测试接口
2. 查看失败的请求
3. 检查请求参数是否正确
4. 检查请求头是否正确
5. 查看响应状态码和错误信息
6. 检查后端服务是否正常运行

#### 6.2.2 跨域问题

**排查步骤：**
1. 检查后端是否配置 CORS
2. 检查网络请求响应头中的 CORS 相关信息
3. 配置 Vite 代理
4. 使用同源部署

### 6.3 性能调试

#### 6.3.1 页面加载慢

**排查步骤：**
1. 使用 Postman 或 curl 测试接口
2. 查看 Waterfall 图
3. 找出加载时间最长的资源
4. 优化图片大小
5. 启用资源压缩
6. 使用 CDN 加速

#### 6.3.2 接口响应慢

**排查步骤：**
1. 检查后端日志中的响应时间
2. 分析慢查询日志
3. 检查向量检索效率
4. 优化模型推理参数
5. 考虑增加缓存

## 7. 相关文档

- [测试指南文档](./测试指南.md) - 学习测试编写和执行
- [技术设计文档](./技术设计.md) - 了解系统架构
- [部署说明文档](./部署说明.md) - 了解部署步骤
- [API接口文档](./API接口文档.md) - 查看接口详细说明
- [操作界面文档](./操作界面.md) - 了解前端界面设计

希望这份本地调试文档能帮助你快速上手开发调试！
